<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="VLAD-Grasp: Zero-shot Grasp Detection via Vision-Language Models">
  <meta property="og:title" content="VLAD-Grasp"/>
  <meta property="og:description" content="VLAD-Grasp: Zero-shot Grasp Detection via Vision-Language Models"/>
  <meta property="og:url" content="mkulshrestha.github.io/vlad"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->


  <meta name="twitter:title" content="VLAD-Grasp">
  <meta name="twitter:description" content="VLAD-Grasp: Zero-shot Grasp Detection via Vision-Language Models">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Vision Language Models Robotics Zero-shot Grasping">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>VLAD-Grasp</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">VLAD-Grasp: Zero-shot Grasp Detection via Vision-Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.linkedin.com/in/manav-kulshrestha/" target="_blank">Manav Kulshrestha</a>,</span>
              <span class="author-block">
                <a href="https://stalhabukhari.github.io/" target="_blank">S. Talha Bukhari</a>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=laN6vmAAAAAJ&hl=en" target="_blank">Damon Conover</a>,</span>
              <span class="author-block">
                <a href="https://www.cs.purdue.edu/homes/ab/" target="_blank">Aniket Bera</a></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Purdue University<br>(Under Review)</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://scholar.google.com/" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming soon)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://scholar.google.com/" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Robotic grasping is a fundamental capability for autonomous manipulation; however, most existing methods rely on large-scale expert annotations and necessitate retraining to handle new objects. We present VLAD-Grasp, a Vision-Language model Assisted zero-shot approach for Detecting grasps. From a single RGB-D image, our method (1) prompts a large visionâ€“language model to generate a goal image where a straight rod "impales" the object, representing an antipodal grasp, (2) predicts depth and segmentation to lift this generated image into 3D, and (3) aligns generated and observed object point clouds via principal component analysis and correspondence-free optimization to recover an executable grasp pose. Unlike prior work, our approach is training-free and does not rely on curated grasp datasets. Despite this, VLAD-Grasp achieves performance that is competitive with or superior to that of state-of-the-art supervised models on the Cornell and Jacquard datasets. We further demonstrate zero-shot generalization to novel real-world objects on a Franka Research 3 robot, highlighting visionâ€“language foundation models as powerful priors for robotic manipulation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Website Under Construction Section -->
<section class="hero is-warning is-bold">
  <div class="hero-body">
    <div class="container has-text-centered">
      <h1 class="title is-1" style="font-size: 3.5rem;">
        ðŸš§ WEBSITE UNDER CONSTRUCTION ðŸš§
      </h1>
      <h2 class="subtitle is-3">
        Check back soon for updates and demos.
      </h2>
    </div>
  </div>
</section>
<!-- End Website Under Construction Section -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{kulshrestha2025vlad,
        title={VLAD-Grasp: Vision-Language Model Assisted Zero-Shot Grasp Detection}, 
        author={Kulshrestha, Manav and Bukhari, S Talha and Conover, Damon and Bera, Aniket},
        year={2025},
        journal={arXiv preprint arXiv:_}
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->

</body>
</html>
